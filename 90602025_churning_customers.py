# -*- coding: utf-8 -*-
"""90602025_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ImD8xCeifwBbEeSQ78q8BpJ9FMP4KNlV

**Title:** *Churning Customers in a Telecoms Company*

**Description**: *Customer churn is a major problem and one of the most important concerns for large companies. This code will develop a churn prediction model that assists telecom operators in predicting customers who are most likely subject to churn, thus taking necessary actions to reduce this churn*

**Author:** *Faisal Alidu*

**Date:** *16th November 2023*
"""

## Mounting on Google drive
from google.colab import drive
drive.mount('/content/drive')

## Import libraries
import pickle
import warnings
import numpy as np
import pandas as pd
from math import sqrt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Model
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import plotly.express as px ##  For visualization
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt ##  For visualization
from tensorflow.keras.layers import Input, Dense, Dropout
from sklearn.metrics import accuracy_score, roc_auc_score
from imblearn.over_sampling import SMOTE ## For oversampling
from sklearn.feature_selection import RFECV ## For feature selection
from sklearn.model_selection import StratifiedKFold ## For feature selection
from sklearn.linear_model import LogisticRegression ## For feature selection
warnings.simplefilter(action='ignore', category=Warning) ## Suppress warnings
from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,roc_curve, recall_score,
                                  classification_report, f1_score, precision_recall_fscore_support)

## To display all the columns of the dataset:
pd.set_option('display.max_columns', None)

## Read the dataset
churn_dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro to AI Folder/Assignment 3/CustomerChurn_dataset.csv')

## View the dataset after reading
churn_dataset

## View the datatypes for each columns in the dataset
churn_dataset.dtypes

## View the number of rows and columns in the dataset
churn_dataset.shape

## The describe function gives information about only numerical columns.
churn_dataset.describe()

## The info() function gives some details like the total number of respective datatypes in the dataset
churn_dataset.info()

## Checking if there are Null Values in my dataset
churn_dataset.isnull().sum()

"""### Making some changes to the churn_dataset"""

## Renaming column: gender to Gender
churn_dataset.rename(columns={'gender':'Gender', }, inplace=True)

## Renaming column: tenure to Tenure
churn_dataset.rename(columns={'tenure':'Tenure', }, inplace=True)

## Convert the data type of the 'TotalCharges' column from object to numeric
churn_dataset['TotalCharges'] = churn_dataset['TotalCharges'].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()

## Convert 'No internet service' to 'No' for the below mentioned columns
cols = ['OnlineBackup','StreamingMovies','DeviceProtection',
                'TechSupport','OnlineSecurity','StreamingTV']
for i in cols :
    churn_dataset[i]  = churn_dataset[i].replace({'No internet service' : 'No'})

## Convert 'No phone service' to 'No' for the below mentioned column
churn_dataset['MultipleLines'] = churn_dataset['MultipleLines'].replace({'No phone service' : 'No'})

## View the number of unique values of each column for the 7 columns which some of their unique values were changed
df = churn_dataset[['MultipleLines','OnlineBackup','StreamingMovies','DeviceProtection','TechSupport','OnlineSecurity','StreamingTV']]
print("\nUnique values:")
print(df.nunique())

## View the number of unique values each column holds in churn_dataset
churn_dataset.nunique()

## View the datatypes for each columns in the dataset
churn_dataset.dtypes

## View the dataset after making changes to these columns: Gender, Tenure and TotalCharges
churn_dataset.head()

## Confirm that the data type of 'TotalCharges' is numeric
churn_dataset['TotalCharges'].dtypes

## Checking for the number of missing values in 'TotalCharges' column
churn_dataset['TotalCharges'].isnull().sum()

## The info() function gives some details like the total number of respective datatypes in the dataset
churn_dataset.info()

"""### **Exploratory Data Analysis**
*Even without building a fancy machine learning model, a simple data-driven analysis like this can help organizations understand why they are losing customers and what they can do about it. For instance, if the company realizes that most of their users who churn have not signed up for tech support, they can include this as a complimentary service in some of their future product offerings to prevent other customers from leaving.*
"""

## An 0verview of the data
def data_overiew(df, message):
    print(f'{message}:')
    print('\n\nNumber of rows: ', df.shape[0])
    print("Number of features:", df.shape[1])
    print("\nData Features:")
    print(df.columns.tolist())
    print("\nMissing values:",df.isnull().sum().values.sum())
    print("\n\nUnique values:")
    print(df.nunique())

data_overiew(churn_dataset, 'Overview of the dataset')

## Count the number of customers in the dataset who have churned
churn_dataset["Churn"].value_counts()

## Exploring the target variable; Churn
target_var = churn_dataset["Churn"].value_counts().to_frame()
target_var = target_var.reset_index()
target_var = target_var.rename(columns={'index': 'Category'})
fig = px.pie(target_var, values='Churn', names='Category', color_discrete_sequence=[" blue", "red"], title='Distribution of Churn (in terms of Yes and No)')
fig.show()

## Analyzing some demographic data points
some_cols = ['Gender','SeniorCitizen',"Partner","Dependents"]

plt.figure(figsize=(20,4))

for i, col in enumerate(some_cols):
    ax = plt.subplot(1, len(some_cols), i+1)
    sns.countplot(x=str(col), data=churn_dataset)
    ax.set_title(f"{col}")

"""*From the visual above, most customers in the dataset are younger
individuals without a dependent. There is an equal distribution
of user gender and nearly equal distribution of marital status*
"""

## Check the relationship between customer churn and costs/ Monthly Charges:
sns.boxplot(x='Churn', y='MonthlyCharges', data=churn_dataset)

"""*In the real world, users tend to unsubscribe to a service such as their mobile service provider and switch to a different brand if they find the monthly subscription cost too high.*

*From the visual above, the assumption above is true. Customers who churned have a higher median monthly charge than customers who renewed their subscription.*
"""

## Analyzing the relationship between customer churn and a few other categorical variables captured in the dataset
cols = ['InternetService',"TechSupport","OnlineBackup","Contract"]

plt.figure(figsize=(14,4))

for i, col in enumerate(cols):
    ax = plt.subplot(1, len(cols), i+1)
    sns.countplot(x ="Churn", hue = str(col), data = churn_dataset)
    ax.set_title(f"{col}")

"""**InternetService:** *It is clear from the visual above that customers who use fiber optic Internet churn more often than other users. This might be because fiber Internet is a more expensive service, or this provider doesnâ€™t have good coverage.*

**TechSupport:** *Many users who churned did not sign up for tech support. This might mean that these customers did not receive any guidance on fixing technical issues and decided to stop using the service.*

**OnlineBackup:** *Many customers who had churned did not sign up for an online backup service for data storage.*

**Contract:** *Users who churned were almost always on a monthly contract. This makes sense, since these customers pay for the service on a monthly basis and can easily cancel their subscription before the next payment cycle.*

### Data Preprocessing And Feature Extraction
"""

## Drop 'customerID' column since it is not useful for churn prediction as the feature is used for identification of customers.
selected_features = churn_dataset.drop(['customerID'], axis = 1)

## View the selected_features dataset
selected_features

## View the number of rows and columns in the selected_features dataset
selected_features.shape

## The info() function gives some details like the total number of respective datatypes
selected_features.info()

## Extract Categorical features from the selected features
selected_features_cat = selected_features.select_dtypes(np.object)

## View the categorical features of the selected_features dataset
selected_features_cat

## View the datatype of the categorical features of selected features dataset. They are all objects
selected_features_cat.dtypes

## View the number of rows and columns of the categorical features of selected features dataset
selected_features_cat.shape

## View the columns that are categorical in the selected_features dataset
selected_features_cat.columns

## Categorical features that will need to be converted
Gender = selected_features['Gender']
Partner = selected_features['Partner']
Dependents = selected_features['Dependents']
PhoneService = selected_features['PhoneService']
MultipleLines = selected_features['MultipleLines']
InternetService = selected_features['InternetService']
OnlineSecurity = selected_features['OnlineSecurity']
OnlineBackup = selected_features['OnlineBackup']
DeviceProtection = selected_features['DeviceProtection']
TechSupport = selected_features['TechSupport']
StreamingTV = selected_features['StreamingTV']
StreamingMovies = selected_features['StreamingMovies']
Contract = selected_features['Contract']
PaperlessBilling = selected_features['PaperlessBilling']
PaymentMethod = selected_features['PaymentMethod']
Churn = selected_features['Churn']

## Drop all categorical features from the selected features
selected_features.drop(selected_features_cat.columns, axis=1, inplace=True)

## View the selected features after dropping all categorical features
selected_features.head()

## Checking for the number of missing values in 'TotalCharges' column before imputing missing values
selected_features['TotalCharges'].isnull().sum()

## Impute missing values for 'TotalCharges' column
selected_features['TotalCharges'] = selected_features['TotalCharges'].fillna(selected_features['TotalCharges'].median())

## Checking for the number of missing values in 'TotalCharges' column after imputing missing values
selected_features['TotalCharges'].isnull().sum()

## Perform Feature Scaling on 'tenure', 'MonthlyCharges' in order to bring them on same scale
standardScaler = StandardScaler()
columns_for_scaling = ['SeniorCitizen','Tenure', 'MonthlyCharges', 'TotalCharges']

## Apply the feature scaling operation on dataset using fit_transform() method
selected_features[columns_for_scaling] = standardScaler.fit_transform(selected_features[columns_for_scaling])

## View the selected feature after scaling the numeric columns
selected_features.head()

## Convert all categorical features into numerical features
Gender = pd.get_dummies(Gender, prefix='Gender')
Partner = pd.get_dummies(Partner, prefix='Partner')
Dependents = pd.get_dummies(Dependents, prefix='Dependents')
PhoneService = pd.get_dummies(PhoneService, prefix='PhoneService')
MultipleLines = pd.get_dummies(MultipleLines, prefix='MultipleLines')
InternetService = pd.get_dummies(InternetService, prefix='InternetService')
OnlineSecurity = pd.get_dummies(OnlineSecurity, prefix='OnlineSecurity')
OnlineBackup = pd.get_dummies(OnlineBackup, prefix='OnlineBackup')
DeviceProtection = pd.get_dummies(DeviceProtection, prefix='DeviceProtection')
TechSupport = pd.get_dummies(TechSupport, prefix='TechSupport')
StreamingTV = pd.get_dummies(StreamingTV, prefix='StreamingTV')
StreamingMovies = pd.get_dummies(StreamingMovies, prefix='StreamingMovies')
Contract = pd.get_dummies(Contract, prefix='Contract')
PaperlessBilling = pd.get_dummies(PaperlessBilling, prefix='PaperlessBilling')
PaymentMethod = pd.get_dummies(PaymentMethod, prefix='PaymentMethod')

## View Gender after conversion
Gender.head()

## View Partner after conversion
Partner.head()

## View Dependents after conversion
Dependents.head()

## View PhoneService after conversion
PhoneService.head()

## View MultipleLines after conversion
MultipleLines.head()

## View InternetService after conversion
InternetService.head()

## View OnlineSecurity after conversion
OnlineSecurity.head()

## View OnlineBackup after conversion
OnlineBackup.head()

## View DeviceProtection after conversion
DeviceProtection.head()

## View TechSupport after conversion
TechSupport.head()

## View StreamingTV after conversion
StreamingTV.head()

## View StreamingMovies after conversion
StreamingMovies.head()

## View Contract after conversion
Contract.head()

## View PaperlessBilling after conversion
PaperlessBilling.head()

## View PaymentMethod after conversion
PaymentMethod.head()

# Concatenating the transformed categorical columns to selected features dataset which is already keeping numeric some columns to form fully numeric columns
selected_features = pd.concat([selected_features, Gender, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection,
                               TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod, Churn], axis=1)

## Perform Label encoding on 'Churn' column to map No to 0 and Yes to 1
label_encoder = LabelEncoder()
selected_features['Churn'] = label_encoder.fit_transform(selected_features['Churn'])

## View the churn column to confirm that the categorical column has been converted to numerical
selected_features['Churn'].head()

## View the selected_features dataset after categorical features have been converted into numeric
selected_features.head()

## View the number of rows and columns of the selected_features dataset after categorical features have been converted into numeric
selected_features.shape

## View the datatypes for each columns after categorical features have been converted into numeric
selected_features.dtypes

## View the column names of the selected_features dataset after categorical features have been converted into numeric
selected_features.columns

"""### Feature Selection"""

## Select the the independent (X) and dependent (y) variables from the selected_features dataset
y = selected_features['Churn']
X = selected_features.drop(['Churn'], axis = 1)

X.head()

X.shape

y.head()

## Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

## Feature selection to improve model building
log = LogisticRegression()
rfecv = RFECV(estimator=log, cv=StratifiedKFold(10, random_state=50, shuffle=True), scoring="accuracy")
rfecv.fit(X, y)

## Recursive Feature Elimination (REF)
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])
plt.grid()
plt.xticks(range(1, X.shape[1]+1))
plt.xlabel("Number of Selected Features")
plt.ylabel("CV Score")
plt.title("Recursive Feature Elimination (RFE)")
plt.show()

print("The optimal number of features: {}".format(rfecv.n_features_))

## Saving dataframe with optimal features
X_rfe = X.iloc[:, rfecv.support_]

## Overview of the optimal features in comparison with the intial dataframe
print("\nInitial dimension of X: {}".format(X.shape))
print("\nInitial X column list:", X.columns.tolist())
print("\nDimension of X considering only the optimal features: {}".format(X_rfe.shape))
print("\nColumn list of X considering only the optimal features:", X_rfe.columns.tolist())

"""### Training and Model Creation"""

## The dataset containing optimal features
final_selected_features = X_rfe.columns.tolist()

final_selected_features

## Select the the independent (X) and dependent (y) variables
X = selected_features[final_selected_features]
y = selected_features['Churn']

X.head()

X.shape

y.shape

## Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

## View the dimension of the training data and testing data
X_train.shape, y_test.shape, y_train.shape, X_test.shape

"""### **Oversampling**
*The dataset is imbalanced. Most customers in the dataset did not*
*churn - only 27% (approximately) of them did.This class imbalance problem*
*can lead to an underperforming machine learning model which is always*
*predicting a single outcome. We will use a technique called oversampling.*
*This is a process that involves randomly selecting samples from the*
*minority class and adding it to the training dataset. We are going to oversample the minority class until the number of data points are equal to*
*that of the majority class. We will oversample solely on the training dataset*
"""

## Oversampling the training dataset
oversample = SMOTE(k_neighbors=5)
X_smote, y_smote = oversample.fit_resample(X_train, y_train)
X_train, y_train = X_smote, y_smote

## Check the number of samples in each class to ensure that they are equal:
##There should be 4,138 values in each class, which means that the training dataset is now balanced.
y_train.value_counts()

## View the dimension of the training data and testing data
X_train.shape, y_test.shape, y_train.shape, X_test.shape

## Define the architecture of the neural network using the Functional API
input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(64, activation='relu')(input_layer)
dropout_1 = Dropout(0.5)(hidden_layer_1)
hidden_layer_2 = Dense(32, activation='relu')(dropout_1)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

## Create the model
model = Model(inputs=input_layer, outputs=output_layer)

## Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

## Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))

## Evaluate the model
y_pred = model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

## Calculate accuracy and AUC score
accuracy = accuracy_score(y_test, y_pred_binary)
auc_score = roc_auc_score(y_test, y_pred)

print(f"\nAccuracy: {accuracy}")
print(f"\nAUC Score: {auc_score}")

## Evaluate the model's accuracy and calculate the AUC value.
y_pred = model.predict(X_test)
predictions = [np.round(value) for value in y_pred]

## Evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

## Calculate the AUC


fpr, tpr, thresholds = roc_curve(y_test, predictions)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.001, 1])
plt.ylim([0, 1.001])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show();

## Calculating the confidence factor
cofidence_factor = 2.58 * sqrt( (accuracy * (1 - accuracy)) / y_test.shape[0])
cofidence_factor

## Save the model using pickle
filename = 'customer_churn.pkl'
pickle.dump(model, open(filename, 'wb'))

loaded_model = pickle.load(open(filename, 'rb'))

y_pred = loaded_model.predict(X_test)
predictions = [np.round(value) for value in y_pred]

## Evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))